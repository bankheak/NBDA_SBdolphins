if(!require(sp)){install.packages('sp'); library(sp)}
if(!require(OpenStreetMap)){install.packages('OpenStreetMap'); library(OpenStreetMap)}
if(!require(rgdal)){install.packages('rgdal'); library(rgdal)}
if(!require(devtools)){install.packages('devtools'); library(devtools)} # To load NBDA
if(!require(asnipe)){install.packages('asnipe'); library(asnipe)} # get_group_by_individual
if(!require(sf)){install.packages('sf'); library(sf)} # Convert degrees to meters
if(!require(sp)){install.packages('sp'); library(sp)} # Convert degrees to meters
if(!require(adehabitatHR)){install.packages('adehabitatHR'); library(adehabitatHR)} # Caluculate MCPs and Kernel density
if(!require(kinship2)){install.packages('kinship2'); library(kinship2)} # genetic relatedness
setwd("../../NBDA")
load_all()
setwd("~/GitHub/NBDA_SBdolphins/Code")
# all networks available under https://datadryad.org/review?doi=doi:10.5061/dryad.sc26m6c.
setwd("../Data") # set working directory
source("../Code/functions.R") # nxn
# Read in matrix
SRI_vert_all <- readRDS("SRI_vert_all.RData")
# Read nxn
SRI_hor_no_vert_all <- readRDS("SRI_hor_no_vert_all.RData")
# Read ecol data
ecol_all <- readRDS("ecol_all.RData")
# Read ILVs
ILV_all <- read.csv("ILV_dem.csv", header=TRUE, sep=",")
ILV_all <- ILV_all[, c("Alias", "HI_Indiv", "Mom", "Sex", "BirthYear")]
# Read orig_data
orig_data <- read.csv("orig_data.csv")
orig_data <- subset(orig_data, Code %in% ILV_all$Alias)
orig_data$Confirmed_HI <- ifelse(orig_data$ConfHI != "0", 1, 0)
# Create demonstrator column
ILV_all$Demons_HI_forage <- ifelse(
ILV_all$Alias %in% unique(orig_data$Code[orig_data$Confirmed_HI == 1 & orig_data$Year == 1995]),
"yes",
"no"
)
# Create acquisition data
# Step 1: Filter orig_data for confirmed HI behavior after 1995
hi_data <- orig_data[orig_data$Confirmed_HI == 1 & orig_data$Year > 1995, ]
# Step 2: Get the first year each Alias showed the behavior
first_hi_year <- aggregate(Year ~ Code, data = hi_data, FUN = min)
# Step 3: Create a new column for order of acquisition
first_hi_year$HI_Order_acquisition <- first_hi_year$Year - 1995
# Step 4: Merge this info back into ILV_all
ILV_all <- merge(ILV_all, first_hi_year[, c("Code", "HI_Order_acquisition")],
by.x = "Alias", by.y = "Code", all.x = TRUE)
# Step 5: Replace NA with 0 for individuals who had the behavior in 1995
ILV_all$HI_Order_acquisition[ILV_all$Demons_HI_forage == "yes"] <- 0
# Extract Confirmed_HI (learners and demonstrators)
Confirmed_HI <- subset(ILV_all, subset = ILV_all$HI_Indiv == 1)
Confirmed_HI <- Confirmed_HI[order(Confirmed_HI$HI_Order_acquisition),]
# get ID codes of all HI
HI_all <- Confirmed_HI$Alias
# extract IDs of all HI that are treated as learners
HI_learners <- as.vector(subset(Confirmed_HI$Alias, subset=Confirmed_HI$Demons_HI_forage=="no"))
# extract IDs of all HI treated as demonstrators
HI_demons <- as.vector(subset(Confirmed_HI$Alias, subset=Confirmed_HI$Demons_HI_forage=="yes"))
# extract ID names from data file
IDs <- sort(as.vector(unique(orig_data[,"Code"])))
# extract order of acquisition
order <- NULL # create an object to store the vector of acquisition
for (i in 1:length(HI_learners)){ # for each sponger, extract the position in the networks and ILV data frame
order[i] <- which(IDs==HI_learners[i])
}
order <- as.vector(order)
OAc <- order
# extract positions of demonstrators
demons <- NULL # create an object to store the vector of acquistion
for (i in 1:length(HI_demons)){ # for each HI demonstrator, extract the position in the networks and ILV data frame
demons[i] <- which(IDs==HI_demons[i])
}
# contains positions of all HU demonstrators
demons <- as.vector(demons)
# create vector of length(IDs) with 0 for non-demonstrators and 1 for demonstrators
demons_vector <- c(rep(0,length(IDs)))
for (i in demons){
demons_vector[i] <- 1
}
## prepare individual-level variables
Sex <- ifelse(ILV_all$Sex == "Female", 1, 0)
ILV_all$BirthYear <- as.numeric(ILV_all$BirthYear)
Age <- ifelse(is.na(ILV_all$BirthYear), 1985, ILV_all$BirthYear)
n.assMatrix <- 3 # number of matrices
assMatrix.B <- array(data = c(SRI_vert_all, SRI_hor_no_vert_all, ecol_all
#, relate
), dim=c(nrow(SRI_vert_all), ncol(SRI_vert_all), n.assMatrix)) # create an array with the four matrices
Sex <- matrix(data = Sex, nrow=length(IDs), byrow=F) # all ILVs need to go into a matrix
Age <- matrix(data = Age, nrow=length(IDs), byrow=F)
ILVs <- c("Sex","Age")
label <- "HIC"
# extract the Confirmed_HI learners with no maternity data available
HI_filter <- subset(Confirmed_HI, subset=Confirmed_HI$Demons_HI_forage=="no")
HI_filter2 <- sort(subset(HI_filter$id_individual, subset=is.na(HI_filter$Mom)))
vec <- NULL
for (i in 1:length(HI_filter2)){
a <- which(IDs==HI_filter2[i])
vec[i] <- a
} # get position of spongers with no maternity data
filter <- paste0(label,"_", vec)
# create NBDA Data Object
nbdaDataHI.C <- nbdaData(label=label, assMatrix=assMatrix.B, asoc_ilv=ILVs,
int_ilv=ILVs, multi_ilv=ILVs, orderAcq=OAc,asocialTreatment="constant",
demons = demons_vector) # creates OADA object
# apply filter to exclude individuals without maternity data as learners
nbdaDataHI.C.filter <- filteredNBDAdata(nbdadata=nbdaDataHI.C, filter="id", exclude=filter)
# check ILVs for asoc (ILVs only influence asocial learning), int (ILVs influence asocial and social learning independently)
# and multi (ILVs influence both asocial and social learning to the same extent).
# in this analysis, multiILV will be set to 0 and therefore not estimated (in the constraintsVectMatrix)
nbdaDataHI.C.filter@asoc_ilv
nbdaDataHI.C.filter@int_ilv
nbdaDataHI.C.filter@multi_ilv
# set number of networks and number of ILVs
num_networks <- 3
num_ILVs <- 2
vector <- seq(1:(num_networks+(2*num_ILVs))) # create a vector for the full model with all networks and ILVs (excluding multiILV slots which will all be set to 0)
count <- 0 # create an object 'count', which starts on 0
constraintsVect <- matrix(nrow = 10000000, ncol=(num_networks+(2*num_ILVs))) # create a matrix to save the combination of parameters in
constraintsVect[1,] <- seq(1:(num_networks+(2*num_ILVs))) # the first row gets filled with a sequence from 1:12 (all parameters will be estimated, none are set to 0)
for (i in 1:(num_networks+(2*num_ILVs)-1)){ # a loop for each number of parameters to be estimated
array <- combn(vector, i, FUN = NULL, simplify = TRUE) # for each number of paramters to be estiamted (e.g. 2) create all possible combinations of numbers between 1:7 (e.g. 2&7, 1&5 etc)
for (j in 1:length(array[1,])){ # for each of those combinations
vector2 <- seq(1:((num_networks+(2*num_ILVs))-i)) # create a second vector with 6-i free spaces
position <- array[,j] # for each created combination
count <- count+1 # add +1 to the count
for (k in position){ # at each possible position
vector2 <- append(vector2, 0, after=k-1) # add a 0 (e.g. 1 0 2 3 ...; 1 2 0 3 4 5 ...; 1 2 3 0 4 5 ....)
}
constraintsVect[count+1,] <- vector2 # and save the resulting order in a matrix
}
}
constraintsVect <- na.omit(constraintsVect) # remove all NAs from the matrix
constraintsVect <- rbind(constraintsVect, rep.int(0,(num_networks+2*(num_ILVs)))) # add a last row with all 0
constraintsVect <- cbind(constraintsVect, matrix(0,ncol=num_ILVs, nrow=length(constraintsVect[,1]))) ## add 2 columns at the end with all 0 (multi_ILV)
constraintsVectMatrix<-constraintsVect
# Each line of the resulting object specifies a model
# Each element in the line corresponds to a parameter in the model. When an element is zero, that paramter is constrained
# =0. When two elements have the same value, they are constrained to have the same value (not relevant here).
# For example:
constraintsVectMatrix[1,]
# run NBDA using the NBDA Data object and the constraitnsVectMatrix
# this fits every model specified by the constrainstsVectMatrix matrix
tableHI.C.filter<-oadaAICtable(nbdadata=nbdaDataHI.C.filter, constraintsVectMatrix=constraintsVectMatrix,writeProgressFile = T)
print(tableHI.C.filter)
save(tableSPONGING.C.filter, file="AIC table HI.Rdata")
save(tableHI.C.filter, file="AIC table HI.Rdata")
load("AIC table HI.Rdata")
write.csv(as.data.frame(tableHI.C.filter@printTable), "AIC table sponging.csv")
##Create a new object with a printTable that excludes unfitted model
newTableSPONGING<-tableHI.C.filter
##Create a new object with a printTable that excludes unfitted model
tableHI.C.filter<-tableHI.C.filter
newTableHI@printTable<-tableHI.C.filter@printTable[!is.nan(tableHI.C.filter@printTable$aicc)&!is.na(tableHI.C.filter@printTable$aicc),]
load("AIC table HI.Rdata")
write.csv(as.data.frame(tableHI.C.filter@printTable), "AIC table sponging.csv")
##Create a new object with a printTable that excludes unfitted model
newTableHI<-tableHI.C.filter
newTableHI@printTable<-tableHI.C.filter@printTable[!is.nan(tableHI.C.filter@printTable$aicc)&!is.na(tableHI.C.filter@printTable$aicc),]
tableHI.C.filter@aicc<-tableHI.C.filter@aicc[!is.nan(tableHI.C.filter@aicc)&!is.na(tableHI.C.filter@aicc)]
tableHI.C.filter@MLEs<-tableHI.C.filter@MLEs[!is.nan(tableHI.C.filter@aicc)&!is.na(tableHI.C.filter@aicc),]
newTableHI@MLEilv<-tableHI.C.filter@MLEilv[!is.nan(tableHI.C.filter@aicc)&!is.na(tableHI.C.filter@aicc),]
newTableHI@MLEint<-tableHI.C.filter@MLEint[!is.nan(tableHI.C.filter@aicc)&!is.na(tableHI.C.filter@aicc),]
newTableHI@printTable<-newTableHI@printTable[order(newTableHI@printTable$aicc),]
newTableHI@printTable$deltaAICc<-newTableHI@printTable$aicc-newTableHI@printTable$aicc[1]
# calculate support for fitted models
newTableHI@printTable$RelSupport<- exp(-0.5*newTableHI@printTable$deltaAICc)
newTableHI@printTable$AkaikeWeight<-newTableHI@printTable$RelSupport/sum(newTableHI@printTable$RelSupport)
newTableHI@RelSupport<- exp(-0.5*newTableHI@deltaAIC)
newTableHI@AkaikeWeight<-newTableHI@RelSupport/sum(newTableHI@RelSupport)
dim(tableHI.C.filter@printTable)[1]-dim(newTableHI@printTable)[1]
dim(tableHI.C.filter@printTable)[1]
# save reduced AIC table as csv
write.csv(newTableHI@printTable, file="HI.AIC_table_CORRECTED.csv")
# obtain network support for each network combination
networksSupport_HI<-networksSupport(newTableHI)
networksSupport_HI
write.csv(networksSupport_HI, file="networksSupport_HI.csv")
# extract support for each variable
variable_support <- variableSupport(newTableHI, includeAsocial = T)
variable_support
write.csv(variable_support, file="variable_support_HI.csv")
# extract model averaged medians
MLE_med  <- modelAverageEstimates(newTableHI,averageType = "median")
MLE_med
write.csv(MLE_med, "MLE_HI.csv")
print(newTableHI)[1:10,]
bestModelData<-constrainedNBDAdata(nbdadata=nbdaDataHI.C.filter,constraintsVect =constraintsVectMatrix[4076,])
print(newTableHI)[1:7,]
print(newTableHI)
bestModelData<-constrainedNBDAdata(nbdadata=nbdaDataHI.C.filter,constraintsVect =constraintsVectMatrix[128,])
constraintsVectMatrix[128,]
View(constraintsVect)
View(constraintsVectMatrix)
bestModelData<-constrainedNBDAdata(nbdadata=nbdaDataHI.C.filter,constraintsVect=constraintsVectMatrix[128,])
print(newTableHI)[1:10,]
bestModelData<-constrainedNBDAdata(nbdadata=nbdaDataHI.C.filter,constraintsVect=constraintsVectMatrix[112,])
model.best.social<-oadaFit(bestModelData)
model.best.social@outputPar
# [1] 1.233004e+10 -4.840486e+00
model.best.social@optimisation
model.best.social@aicc
# extract profile likelihood. which=1 extracts the first parameter (s parameter for vertical social learning)
plotProfLik(which=1,model=model.best.social1,range=c(0,1e30), resolution=20)
# extract profile likelihood. which=1 extracts the first parameter (s parameter for vertical social learning)
plotProfLik(which=1,model=model.best.social,range=c(0,1e30), resolution=20)
#Zoom in to locate the lower limit for s
plotProfLik(which=1,model=model.best.social,range=c(0,500), resolution=20)
plotProfLik(which=1,model=model.best.social,range=c(30,40), resolution=20)
profLikCI(which=1,model=model.best.social,lowerRange=c(30,40))
eventTable<-NULL
