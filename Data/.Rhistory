n_mats <- ncol(edge_nxn)
n_edges <- nrow(edge_nxn)
# Flatten all values
values <- as.vector(edge_nxn)
# Create group labels (1 to n_mats repeated for each row)
groups <- rep(1:n_mats, each = n_edges)
# Combine into a data frame
edge_data <- data.frame(value = values, group = groups)
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Create the edge_list
edge_list = data.frame(focal = unlist(one),
other = unlist(two),
trial = 1,
assoc = edge_data[, 1],
time = edge_data[, 2])
# Read in full data
filtered_data <- read.csv("filtered_data.csv")
ILV_all <- read.csv("ILV_dem.csv", header=TRUE, sep=",")
ILV_all <- ILV_all[, c("Alias", "HI_Indiv", "Mom", "Sex", "BirthYear")]
# Subset event data to include only edge_list ids
ILV_all <- subset(ILV_all, Alias %in% unique(edge_list$focal))
# Add order of acquisition data
# Create demonstrator column
ILV_all$Demons_HI_forage <- ifelse(
ILV_all$Alias %in% unique(filtered_data$Code[filtered_data$Confirmed_HI == 1 &
filtered_data$Date == min(filtered_data$Date)]),
"yes",
"no"
)
# Create acquisition data
# Step 1: Filter filtered_data for confirmed HI behavior after first date
hi_data <- filtered_data[filtered_data$Confirmed_HI == 1 & filtered_data$Year != min(filtered_data$Year), ]
# Step 2: Get the first year each Alias showed the behavior
first_hi_year <- aggregate(Year ~ Code, data = hi_data, FUN = min)
# Step 3: Create a new column for order of acquisition
first_hi_year$HI_Order_acquisition <- as.numeric(first_hi_year$Year - min(first_hi_year$Year))
# Step 4: Merge this info back into ILV_all
ILV_all <- merge(ILV_all, first_hi_year[, c("Code", "HI_Order_acquisition")],
by.x = "Alias", by.y = "Code", all.x = TRUE)
# Step 5: Replace NA with 0 for individuals who had the behavior in 1995
ILV_all$HI_Order_acquisition[ILV_all$Demons_HI_forage == "yes"] <- 0
# Add end time
ILV_all$t_end <- max(na.omit(ILV_all$HI_Order_acquisition))
# Change individuals who didn't require behavior to t_end +1
ILV_all$time <- ifelse(is.na(ILV_all$HI_Order_acquisition),
max(na.omit(ILV_all$HI_Order_acquisition)) + 1,
ILV_all$HI_Order_acquisition)
# Edit the other needed columns
ILV_all$id <- ILV_all$Alias
ILV_all$trial <- 1
# Get rid of other columns
event_data <- ILV_all[, c("trial", "id", "time", "t_end")]
# Prepare individual-level variables
ILV_all <- subset(ILV_all, Alias %in% unique(edge_list$focal))
ILV_all$Sex <- ifelse(ILV_all$Sex == "Female", 1, 0)
ILV_all$BirthYear <- as.numeric(ILV_all$BirthYear)
ILV_all$BirthYear <- ifelse(is.na(ILV_all$BirthYear), 1985, ILV_all$BirthYear)
# Constant ILVs
ILV_c <- data.frame(id = ILV_all$Alias,
age = ILV_all$Sex)
# Time varying ILVs
ILV_tv <- data.frame(
trial = 1,
id = rep(ILV_all$Alias, each = 20),
time = rep(1:20, times = length(ILV_all$Alias)),
year = rep(1995:2014, times = length(ILV_all$Alias)),
age = rep(1995:2014, times = length(ILV_all$Alias)) -
rep(ILV_all$BirthYear, each = 20)
)
# Separate HI Behaviors
#' BG = Beg: F, G, H
#' SD = Scavenge and Depredation: A, B, C, D, E
#' FG = Fixed Gear Interaction: P
# Change the code using ifelse statements
filtered_data <- subset(filtered_data, Year %in% 1995:2014)
filtered_data <- subset(filtered_data, Code %in% unique(edge_list$focal))
filtered_data_list <- split(filtered_data, filtered_data$Year)
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G", "H"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(filtered_data_list)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
# Categorize ID to Sightings
ID_sight <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
data <- as.data.frame(table(df$Code))
colnames(data) <- c("Code", "Sightings")
# Order data
order_rows <- rownames(nxn[[1]])
# Now reorder the dataframe
data <- data %>%
arrange(match(Code, order_rows))
})
return(IDbehav)
}
IDbehav <- ID_sight(aux)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$Behav <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Sightings", "Behav")
return(df)
})
}
IDbehav_HI <- get_IDHI(c("BG", "FG", "SD"), IDbehav, rawHI_diff)
# Proportion of Sightings spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$Behav) / as.numeric(df$Sightings)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
# Convert list of HIprop vectors into a matrix
HI_matrix <- do.call(cbind, lapply(prob_HI, function(x) x$HIprop))
# Order edge data
edge_list <- edge_list[order(edge_list$time), ]
# Add transmission weights
ILV_tv$weight <- as.vector(t(HI_matrix))
data_list <- import_user_STb(
event_data = event_data,
networks = edge_list
)
# Generate the model
model_full <- generate_STb_model(data_list, gq = T, est_acqTime = T)
# Fit the model
full_fit <- fit_STb(data_list,
model_full,
parallel_chains = 4,
chains = 4,
cores = 4,
iter = 2000,
refresh=1000
)
?set_cmdstan_path
library(cmdstanr)
cmdstanr::cmdstan_version()
# load packages
#if(!require(devtools)){install.packages('devtools'); library(devtools)} # To load NBDA
if(!require(remotes)){install.packages('remotes'); library(remotes)}
remotes::install_github("stan-dev/cmdstanr") # If STBayes doesn't download
if (!cmdstanr::cmdstan_version()) cmdstanr::install_cmdstan(); cmdstanr::set_cmdstan_path(cmdstanr::cmdstan_path())
if (is.null(cmdstanr::cmdstan_version())) cmdstanr::install_cmdstan(); cmdstanr::set_cmdstan_path(cmdstanr::cmdstan_path())
cmdstanr::set_cmdstan_path(cmdstanr::install_cmdstan())
# Fit the model
full_fit <- fit_STb(data_list,
model_full,
parallel_chains = 4,
chains = 4,
cores = 4,
iter = 2000,
refresh=1000
)
# View output
STb_summary(full_fit, digits = 3)
View(edge_list)
View(SRI_vert_all)
unlist(SRI_vert_all)
# Add vertical network to edge_list
edge_vert <- abind(lapply(SRI_vert_all, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
n_mats <- ncol(edge_vert)
n_edges <- nrow(edge_vert)
values <- as.vector(edge_vert)
groups <- rep(1:n_mats, each = n_edges)
edge_data_vert <- data.frame(value = values, group = groups)
# Add vertical network to edge_list
upper_tri <- lapply(SRI_vert_all, function(df) upper.tri(df, diag = TRUE))
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Create the edge_list
edge_list_vert = data.frame(focal = unlist(one),
other = unlist(two),
trial = 1,
vert = edge_data_vert[, 1],
time = edge_data_vert[, 2])
edge_list <- merge(edge_list, edge_list_vert, all = T)
View(edge_list)
# Posterior Predictive Checks
# Cumulative distribution curve
plot_data_obs <- event_data %>%
filter(time > 0, time <= t_end) %>% # exclude demonstrators (time == 0) and censored (time > t_end)
group_by(trial) %>%
arrange(time, .by_group = TRUE) %>%
mutate(
cum_prop = row_number() / n(),
type = "observed"
) %>%
select(trial, time, cum_prop, type) %>%
ungroup()
# add in 0,0 starting point
plot_data_obs <- bind_rows(
plot_data_obs,
plot_data_obs %>%
distinct(trial) %>%
mutate(time = 0, cum_prop = 0, type = "observed")
) %>%
arrange(trial, time)
# plot it
ggplot() +
geom_line(data = plot_data_ppc,
aes(x = time, y = cum_prop,
group = interaction(draw, trial)), alpha = .1) +
geom_line(data = plot_data_obs, aes(x = time, y = cum_prop), linewidth = 1) +
labs(x = "Time", y = "Cumulative proportion informed", color = "Trial") +
theme_minimal()
# build cumulative curves per draw
plot_data_ppc <- ppc_long %>%
group_by(draw, trial, time) %>%
summarise(n = n(), .groups = "drop") %>%
group_by(draw, trial) %>%
arrange(time) %>%
mutate(cum_prop = cumsum(n) / data_list$Q)
# Posterior Predictive Checks
# Cumulative distribution curve
plot_data_obs <- event_data %>%
filter(time > 0, time <= t_end) %>% # exclude demonstrators (time == 0) and censored (time > t_end)
group_by(trial) %>%
arrange(time, .by_group = TRUE) %>%
mutate(
cum_prop = row_number() / n(),
type = "observed"
) %>%
select(trial, time, cum_prop, type) %>%
ungroup()
# add in 0,0 starting point
plot_data_obs <- bind_rows(
plot_data_obs,
plot_data_obs %>%
distinct(trial) %>%
mutate(time = 0, cum_prop = 0, type = "observed")
) %>%
arrange(trial, time)
draws_df <- as_draws_df(full_fit$draws(variables = "acquisition_time", inc_warmup = FALSE))
# pivot longer
ppc_long <- draws_df %>%
select(starts_with("acquisition_time[")) %>%
pivot_longer(
cols = everything(),
names_to = c("trial", "ind"),
names_pattern = "acquisition_time\\[(\\d+),(\\d+)\\]",
values_to = "time"
) %>%
mutate(
trial = as.integer(trial),
ind = as.integer(ind),
draw = rep(1:(nrow(draws_df)),
each = length(unique(.$trial)) * length(unique(.$ind)))
)
# thin sample for plotting
sample_idx <- sample(c(1:max(ppc_long$draw)), 100)
ppc_long <- ppc_long %>% filter(draw %in% sample_idx)
# build cumulative curves per draw
plot_data_ppc <- ppc_long %>%
group_by(draw, trial, time) %>%
summarise(n = n(), .groups = "drop") %>%
group_by(draw, trial) %>%
arrange(time) %>%
mutate(cum_prop = cumsum(n) / data_list$Q)
# add in 0,0 starting point
plot_data_ppc <- bind_rows(
plot_data_ppc,
plot_data_ppc %>%
distinct(trial, draw) %>%
mutate(time = 0, cum_prop = 0, type = "ppc")
) %>%
arrange(trial, time)
# plot it
ggplot() +
geom_line(data = plot_data_ppc,
aes(x = time, y = cum_prop,
group = interaction(draw, trial)), alpha = .1) +
geom_line(data = plot_data_obs, aes(x = time, y = cum_prop), linewidth = 1) +
labs(x = "Time", y = "Cumulative proportion informed", color = "Trial") +
theme_minimal()
# Estimated versus observed
acqdata = extract_acqTime(full_fit, data_list)
ggplot(acqdata, aes(x = observed_time, y = median_time)) +
geom_segment(
aes(x = observed_time, xend = observed_time,
y = median_time, yend = observed_time),
color = "red",
alpha = 0.3) +
geom_point(size = 2) +
geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") +
labs(x = "Observed time", y = "Estimated time") +
theme_minimal()
View(edge_data)
View(event_data)
# Read in all network data
nxn <- readRDS("nxn.RData")
View(nxn)
SRI_vert_all <- readRDS("SRI_vert_all.RData")
ecol_all <- as.array(readRDS("ecol_all.RData"))
# Read in all network data
nxn <- readRDS("nxn.RData")
# Subset nxn
nxn <- nxn[3:22]
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
# Abind nxn
upper_tri <- lapply(nxn, function(df) upper.tri(df, diag = TRUE))
edge_nxn <- abind(lapply(nxn, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
# Create the edge_list
n_mats <- ncol(edge_nxn)
data.frame(
trial = 1,
focal = c("A", "A", "B",
"A", "A", "B",
"A", "A", "B"),
other = c("B", "C", "C",
"B", "C", "C",
"B", "C", "C"),
foveation = c(1, 0, 1, #summarizes from start of obs period to first event
0, 1, 1, #summarizes from first event to second event
0, 0, 1), #summarizes from second event to final event,
time = rep(1:3, each=3)
)
# Read in all network data
nxn <- readRDS("nxn.RData")
# Subset nxn
nxn <- nxn[3:22]
# Put matrices into data frame
edge_list <- do.call(rbind, lapply(seq_along(nxn), function(t) {
mat <- nxn[[t]]
ids <- colnames(mat)  # or rownames(mat), assuming square
upper_idx <- which(upper.tri(mat, diag = TRUE), arr.ind = TRUE)
data.frame(
focal = ids[upper_idx[, 1]],
other = ids[upper_idx[, 2]],
assoc = mat[upper_idx],
time = t
)
}))
View(edge_list)
# Read in full data
filtered_data <- read.csv("filtered_data.csv")
ILV_all <- read.csv("ILV_dem.csv", header=TRUE, sep=",")
ILV_all <- ILV_all[, c("Alias", "HI_Indiv", "Mom", "Sex", "BirthYear")]
# Subset event data to include only edge_list ids
ILV_all <- subset(ILV_all, Alias %in% unique(edge_list$focal))
ILV_all <- read.csv("ILV_dem.csv", header=TRUE, sep=",")
ILV_all <- ILV_all[, c("Alias", "HI_Indiv", "Mom", "Sex", "BirthYear")]
# Subset event data to include only edge_list ids
ILV_all <- subset(ILV_all, Alias %in% unique(edge_list$focal))
# Add order of acquisition data
# Create demonstrator column
ILV_all$Demons_HI_forage <- ifelse(
ILV_all$Alias %in% unique(filtered_data$Code[filtered_data$Confirmed_HI == 1 &
filtered_data$Date == min(filtered_data$Date)]),
"yes",
"no"
)
# Create acquisition data
# Step 1: Filter filtered_data for confirmed HI behavior after first date
hi_data <- filtered_data[filtered_data$Confirmed_HI == 1 & filtered_data$Year != min(filtered_data$Year), ]
# Step 2: Get the first year each Alias showed the behavior
first_hi_year <- aggregate(Year ~ Code, data = hi_data, FUN = min)
# Step 3: Create a new column for order of acquisition
first_hi_year$HI_Order_acquisition <- as.numeric(first_hi_year$Year - min(first_hi_year$Year))
# Step 4: Merge this info back into ILV_all
ILV_all <- merge(ILV_all, first_hi_year[, c("Code", "HI_Order_acquisition")],
by.x = "Alias", by.y = "Code", all.x = TRUE)
# Step 5: Replace NA with 0 for individuals who had the behavior in 1995
ILV_all$HI_Order_acquisition[ILV_all$Demons_HI_forage == "yes"] <- 0
# Add end time
ILV_all$t_end <- max(na.omit(ILV_all$HI_Order_acquisition))
# Change individuals who didn't require behavior to t_end +1
ILV_all$time <- ifelse(is.na(ILV_all$HI_Order_acquisition),
max(na.omit(ILV_all$HI_Order_acquisition)) + 1,
ILV_all$HI_Order_acquisition)
# Edit the other needed columns
ILV_all$id <- ILV_all$Alias
ILV_all$trial <- 1
# Get rid of other columns
event_data <- ILV_all[, c("trial", "id", "time", "t_end")]
ILV_all$Sex <- ifelse(ILV_all$Sex == "Female", 1, 0)
ILV_all$BirthYear <- as.numeric(ILV_all$BirthYear)
ILV_all$BirthYear <- ifelse(is.na(ILV_all$BirthYear), 1985, ILV_all$BirthYear)
View(ILV_all)
# Constant ILVs
ILV_c <- data.frame(id = ILV_all$Alias,
sex = ILV_all$Sex)
# Time varying ILVs
ILV_tv <- data.frame(
trial = 1,
id = rep(ILV_all$Alias, each = 20),
time = rep(1:20, times = length(ILV_all$Alias)),
year = rep(1995:2014, times = length(ILV_all$Alias)),
age = rep(1995:2014, times = length(ILV_all$Alias)) -
rep(ILV_all$BirthYear, each = 20)
)
# Separate HI Behaviors to create weighted HI prop variable
#' BG = Beg: F, G, H
#' SD = Scavenge and Depredation: A, B, C, D, E
#' FG = Fixed Gear Interaction: P
# Change the code using ifelse statements
filtered_data <- subset(filtered_data, Year %in% 1995:2014)
filtered_data <- subset(filtered_data, Code %in% unique(edge_list$focal))
filtered_data_list <- split(filtered_data, filtered_data$Year)
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G", "H"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(filtered_data_list)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
# Categorize ID to Sightings
ID_sight <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
data <- as.data.frame(table(df$Code))
colnames(data) <- c("Code", "Sightings")
# Order data
order_rows <- rownames(nxn[[1]])
# Now reorder the dataframe
data <- data %>%
arrange(match(Code, order_rows))
})
return(IDbehav)
}
IDbehav <- ID_sight(aux)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$Behav <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Sightings", "Behav")
return(df)
})
}
IDbehav_HI <- get_IDHI(c("BG", "FG", "SD"), IDbehav, rawHI_diff)
# Proportion of Sightings spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$Behav) / as.numeric(df$Sightings)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
View(prob_HI)
View(prob_HI[[1]])
View(prob_HI[[6]])
View(prob_HI[[7]])
# Convert list of HIprop vectors into a matrix
HI_matrix <- do.call(cbind, lapply(prob_HI, function(x) x$HIprop))
?do.call
View(prob_HI)
View(prob_HI[[1]])
# Convert list of HIprop vectors into a matrix
HI_matrix <- do.call(cbind, lapply(seq_along(prob_HI), function(t) {
dat <- prob_HI[[t]]
ids <- dat$Code
HIProp <- dat$HIprop
data.frame(
id = ids,
HIProp = HIProp,
time = t
)
}))
# Convert list of HIprop vectors into a matrix
HI_matrix <- do.call(rbind, lapply(seq_along(prob_HI), function(t) {
dat <- prob_HI[[t]]
ids <- dat$Code
HIProp <- dat$HIprop
data.frame(
id = ids,
HIProp = HIProp,
time = t
)
}))
View(HI_matrix)
# Order edge data
edge_list <- edge_list[order(edge_list$time), ]
# Order edge data
edge_list <- edge_list[order(edge_list$time), ]
